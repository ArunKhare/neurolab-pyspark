{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/06 16:09:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/06 16:09:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/06 16:09:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark When Otherwise | SQL Case When Usage\n",
    "1. Using when() otherwise() on PySpark DataFrame.\n",
    " PySpark supports a way to check multiple conditions in sequence and returns a value when the first condition met by using SQL like case when and when().otherwise() expressions, these works similar to “Switch\" and \"if then else\" statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when ,col\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "df2.show()\n",
    "\n",
    "# Using with select() \n",
    "\n",
    "df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
    "                  .when(df.gender == \"F\",\"Female\")\n",
    "                  .when(df.gender.isNull() ,\"\")\n",
    "                  .otherwise(df.gender).alias(\"new_gender\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|name   |gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|James  |M     |60000 |Male      |\n",
      "|Michael|M     |70000 |Male      |\n",
      "|Robert |null  |400000|          |\n",
      "|Maria  |F     |500000|Female    |\n",
      "|Jen    |      |null  |          |\n",
      "+-------+------+------+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|   name|new_gender|\n",
      "+-------+----------+\n",
      "|  James|      Male|\n",
      "|Michael|      Male|\n",
      "| Robert|          |\n",
      "|  Maria|    Female|\n",
      "|    Jen|          |\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. PySpark SQL Case When on DataFrame.\n",
    "# 2.1 Using Case When Else on DataFrame using withColumn() & select()\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "#Using Case When on withColumn()\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\"))\n",
    "df3.show(truncate=False)\n",
    "\n",
    "# 2.2 Using Case When on SQL Expression\n",
    "\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "              \"ELSE gender END as new_gender from EMP\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3. Multiple Conditions using & and | operator\n",
    "\n",
    "df5.withColumn(“new_column”, when((col(“code”) == “a”) | (col(“code”) == “d”), “A”)\n",
    ".when((col(“code”) == “b”) & (col(“amt”) == “4”), “B”)\n",
    ".otherwise(“A1”)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark SQL expr() (Expression ) Function\n",
    "PySpark expr() is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions.\n",
    "Following is syntax of the expr() function.\n",
    "\n",
    "expr(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n",
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Concatenate Columns using || (similar to SQL)\n",
    "#Concatenate columns using || (sql like)\n",
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n",
    "\n",
    "# 2.2 Using SQL CASE WHEN with expr()\n",
    "# PySpark doesn’t have SQL Like CASE WHEN so in order to use this on PySpark DataFrame withColumn() or select(), you should use expr() function with expression as shown below.\n",
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n",
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n",
      "root\n",
      " |-- increment: long (nullable = true)\n",
      " |-- str_increment: string (nullable = true)\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3 Using an Existing Column Value for Expression\n",
    "from pyspark.sql.functions import expr\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date,df.increment,\n",
    "     expr(\"add_months(date,increment)\")\n",
    "  .alias(\"inc_date\")).show()\n",
    "\n",
    "# 2.4 Giving Column Alias along with expr()\n",
    "from pyspark.sql.functions import expr\n",
    "df.select(df.date,df.increment,\n",
    "     expr(\"\"\"add_months(date,increment) as inc_date\"\"\")\n",
    "  ).show()\n",
    "\n",
    "# 2.5 Case Function with expr()\n",
    "df.select(\"increment\",expr(\"cast(increment as string) as str_increment\")) \\\n",
    "  .printSchema()\n",
    "\n",
    "# 2.7 Arithmetic operations\n",
    "df.select(df.date,df.increment,\n",
    "     expr(\"increment + 5 as new_increment\")\n",
    "  ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.8 Using Filter with expr()\n",
    "from pyspark.sql.functions import expr\n",
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.filter(expr(\"col1 == col2\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark lit() – Add Literal or Constant to DataFrame\n",
    "PySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value. Both these functions return Column type as return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "columns= [\"EmpId\",\"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col,lit\n",
    "df2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n",
    "df2.show(truncate=False)\n",
    "from pyspark.sql.functions import when\n",
    "df3 = df2.withColumn(\"lit_value2\", \\\n",
    "        when(col(\"Salary\") >=40000 & col(\"Salary\") <= 50000,lit(\"100\")) \\\n",
    "        .otherwise(lit(\"200\")))\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dob_year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+--------------------+--------+------+------+\n",
      "|name                |dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|James, A, Smith     |2018    |M     |3000  |\n",
      "|Michael, Rose, Jones|2010    |M     |4000  |\n",
      "|Robert,K,Williams   |2010    |M     |4000  |\n",
      "|Maria,Anne,Jones    |2005    |F     |4000  |\n",
      "|Jen,Mary,Brown      |2010    |      |-1    |\n",
      "+--------------------+--------+------+------+\n",
      "\n",
      "root\n",
      " |-- NameArray: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark Convert String to Array Column\n",
    "# Syntax pyspark.sql.functions.split(str, pattern, limit=-1)\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import split, col\n",
    "df2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n",
    "    .drop(\"name\")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: string (nullable = false)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+-----------------+------------+\n",
      "|name            |languagesAtSchool|currentState|\n",
      "+----------------+-----------------+------------+\n",
      "|James,,Smith    |Java,Scala,C++   |CA          |\n",
      "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
      "|Robert,,Williams|CSharp,VB        |NV          |\n",
      "+----------------+-----------------+------------+\n",
      "\n",
      "+----------------+-----------------+------------+\n",
      "|name            |languagesAtSchool|currentState|\n",
      "+----------------+-----------------+------------+\n",
      "|James,,Smith    |Java,Scala,C++   |CA          |\n",
      "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
      "|Robert,,Williams|CSharp,VB        |NV          |\n",
      "+----------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark – Convert array column to a String\n",
    "# concat_ws()\n",
    "# Syntax  concat_ws(sep, *cols)\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "df2 = df.withColumn(\"languagesAtSchool\",\n",
    "   concat_ws(\",\",col(\"languagesAtSchool\")))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
    "spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,\" + \\\n",
    "    \" currentState from ARRAY_STRING\") \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark Replace Column Values in DataFrame\n",
    "Replace Column Value Character by Character\n",
    "\n",
    "You can replace column values of PySpark DataFrame by using SQL string functions regexp_replace(), translate(), and overlay() with Python examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/06 16:49:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n",
      "+---+------------------+-----+\n",
      "|id |address           |state|\n",
      "+---+------------------+-----+\n",
      "|1  |14851 Jeffrey Road|DE   |\n",
      "|2  |43421 Margarita St|NY   |\n",
      "|3  |13111 Siemon Ave  |CA   |\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()\n",
    "\n",
    "# 1. PySpark Replace String Column Values\n",
    "#Replace part of string with another string\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n",
    "  .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+-----+\n",
      "|id |address               |state|\n",
      "+---+----------------------+-----+\n",
      "|1  |14851 Jeffrey Road    |DE   |\n",
      "|2  |43421 Margarita Street|NY   |\n",
      "|3  |13111 Siemon Avenue   |CA   |\n",
      "+---+----------------------+-----+\n",
      "\n",
      "+---+------------------+----------+\n",
      "| id|           address|     state|\n",
      "+---+------------------+----------+\n",
      "|  1|  14851 Jeffrey Rd|  Delaware|\n",
      "|  2|43421 Margarita St|  New York|\n",
      "|  3|  13111 Siemon Ave|California|\n",
      "+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Replace Column Values Conditionally\n",
    "\n",
    "#Replace string column value conditionally\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn('address', \n",
    "    when(df.address.endswith('Rd'),regexp_replace(df.address,'Rd','Road')) \\\n",
    "   .when(df.address.endswith('St'),regexp_replace(df.address,'St','Street')) \\\n",
    "   .when(df.address.endswith('Ave'),regexp_replace(df.address,'Ave','Avenue')) \\\n",
    "   .otherwise(df.address)) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "# 3. Replace Column Value with Dictionary (map)\n",
    "\n",
    "#Replace values from Dictionary\n",
    "stateDic={'CA':'California','NY':'New York','DE':'Delaware'}\n",
    "df2=df.rdd.map(lambda x: \n",
    "    (x.id,x.address,stateDic[x.state]) \n",
    "    ).toDF([\"id\",\"address\",\"state\"])\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "|id |address           |state|\n",
      "+---+------------------+-----+\n",
      "|1  |A485A Jeffrey Rd  |DE   |\n",
      "|2  |4C4BA Margarita St|NY   |\n",
      "|3  |ACAAA Siemon Ave  |CA   |\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Replace Column Value Character by Character\n",
    "\n",
    "#Using translate to replace character by character\n",
    "from pyspark.sql.functions import translate\n",
    "df.withColumn('address', translate('address', '123', 'ABC')) \\\n",
    "  .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----------+\n",
      "|     col1|col2|col3|new_column|\n",
      "+---------+----+----+----------+\n",
      "|ABCDE_XYZ| XYZ| FGH| ABCDE_FGH|\n",
      "+---------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Replace Column with Another Column Value\n",
    "\n",
    "#Replace column with another column\n",
    "from pyspark.sql.functions import expr\n",
    "df = spark.createDataFrame(\n",
    "   [(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], \n",
    "    (\"col1\", \"col2\",\"col3\")\n",
    "  )\n",
    "df.withColumn(\"new_column\",\n",
    "              expr(\"regexp_replace(col1, col2, col3)\")\n",
    "              .alias(\"replaced_value\")\n",
    "              ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|overlayed|\n",
      "+---------+\n",
      "|ABCDE_FGH|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Replace All or Multiple Column Values\n",
    "# 7. Using overlay() Function\n",
    "# Replace column value with a string value from another column.\n",
    "from pyspark.sql.functions import overlay\n",
    "df = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n",
    "df.select(overlay(\"col1\", \"col2\", 7).alias(\"overlayed\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+-----------------------+-------------------+\n",
      "|id |input_timestamp        |timestamp          |\n",
      "+---+-----------------------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|\n",
      "+---+-----------------------+-------------------+\n",
      "\n",
      "+---------------------------------------------------------------+\n",
      "|to_timestamp(06-24-2019 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+---------------------------------------------------------------+\n",
      "|                                            2019-06-24 12:01:19|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Syntax: to_timestamp(timestampString:Column) \n",
    "# Syntax: to_timestamp(timestampString:Column,format:String) \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "# # Using Cast to convert TimestampType to DateType\n",
    "# df.withColumn('timestamp_string', to_timestamp('timestamp').cast('string')) \\\n",
    "#   .show(truncate=False)\n",
    "\n",
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "\n",
    "#SQL string to TimestampType\n",
    "spark.sql(\"select to_timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
    "#SQL CAST timestamp string to TimestampType\n",
    "spark.sql(\"select timestamp('2019-06-24 12:01:19.000') as timestamp\")\n",
    "#SQL Custom string to TimestampType\n",
    "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+----------------------------------------------------------+\n",
      "|to_date(06-24-2019 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+----------------------------------------------------------+\n",
      "|                                                2019-06-24|\n",
      "+----------------------------------------------------------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2023-03-06|\n",
      "+---+-----------------------+----------+\n",
      "\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|id |input_timestamp        |ts                 |datetype  |\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|2019-06-24|\n",
      "+---+-----------------------+-------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date_type: date]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Syntax: to_date(timestamp_column)\n",
    "# Syntax: to_date(timestamp_column,format)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('SparkByExamples.com') \\\n",
    "          .getOrCreate()\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.select(to_date(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "  \n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Timestamp Type to DateType\n",
    "df.withColumn(\"date_type\",to_date(current_timestamp())) \\\n",
    "  .show(truncate=False) \n",
    "\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
    "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
    "  .show(truncate=False)\n",
    "    \n",
    "#SQL TimestampType to DateType\n",
    "spark.sql(\"select to_date(current_timestamp) as date_type\")\n",
    "#SQL CAST TimestampType to DateType\n",
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\")\n",
    "#SQL CAST timestamp string to DateType\n",
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Timestamp String (default format) to DateType\n",
    "spark.sql(\"select to_date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Custom Timeformat to DateType\n",
    "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+------------+-----------------+\n",
      "|current_date|yyyy MM dd|      MM/dd/yyyy|yyyy MMMM dd|   yyyy MMMM dd E|\n",
      "+------------+----------+----------------+------------+-----------------+\n",
      "|  2023-03-06|2023 03 06|03/06/2023 05:09| 2023 Mar 06|2023 March 06 Mon|\n",
      "+------------+----------+----------------+------------+-----------------+\n",
      "\n",
      "+------------+----------+----------------+------------+-----------------+\n",
      "|current_date|yyyy_MM_dd|      MM_dd_yyyy|yyyy_MMMM_dd|   yyyy_MMMM_dd_E|\n",
      "+------------+----------+----------------+------------+-----------------+\n",
      "|  2023-03-06|2023 03 06|03/06/2023 05:09| 2023 Mar 06|2023 March 06 Mon|\n",
      "+------------+----------+----------------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark date_format() – Convert Date to String format\n",
    "\n",
    "# Syntax:  date_format(column,format)\n",
    "# Example: date_format(current_timestamp(),\"yyyy MM dd\").alias(\"date_format\")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
    "df.select(current_date().alias(\"current_date\"), \\\n",
    "      date_format(current_date(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n",
    "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n",
    "   ).show()\n",
    "\n",
    "#SQL\n",
    "\n",
    "spark.sql(\"select current_date() as current_date, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|      date|current_date|datediff|\n",
      "+----------+------------+--------+\n",
      "|2019-07-01|  2023-03-06|    1344|\n",
      "|2019-06-24|  2023-03-06|    1351|\n",
      "|2019-08-24|  2023-03-06|    1290|\n",
      "+----------+------------+--------+\n",
      "\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "| id|      date|datesDiff|  montsDiff|montsDiff_round|         yearsDiff|yearsDiff_round|\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "|  1|2019-07-01|     1344|44.16129032|          44.16|3.6801075266666667|           3.68|\n",
      "|  2|2019-06-24|     1351|44.41935484|          44.42| 3.701612903333333|            3.7|\n",
      "|  3|2019-08-24|     1290|42.41935484|          42.42|3.5349462366666664|           3.53|\n",
      "+---+----------+---------+-----------+---------------+------------------+---------------+\n",
      "\n",
      "+----------+\n",
      "|years_diff|\n",
      "+----------+\n",
      "|     -3.68|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PySpark – Difference between two dates (days, months, years)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('SparkByExamples.com') \\\n",
    "          .getOrCreate()\n",
    "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select(\n",
    "      col(\"date\"),\n",
    "      current_date().alias(\"current_date\"),\n",
    "      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n",
    "    ).show()\n",
    "\n",
    "df.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n",
    "      .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n",
    "      .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n",
    "      .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n",
    "      .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n",
    "      .show()\n",
    "\n",
    "data2 = [(\"1\",\"07-01-2019\"),(\"2\",\"06-24-2019\"),(\"3\",\"08-24-2019\")]  \n",
    "df2=spark.createDataFrame(data=data2,schema=[\"id\",\"date\"])\n",
    "df2.select(\n",
    "    to_date(col(\"date\"),\"MM-dd-yyyy\").alias(\"date\"),\n",
    "    current_date().alias(\"endDate\")\n",
    "    )\n",
    "\n",
    "spark.sql(\"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subjects: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|name   |col               |\n",
      "+-------+------------------+\n",
      "|James  |[Java, Scala, C++]|\n",
      "|James  |[Spark, Java]     |\n",
      "|Michael|[Spark, Java, C++]|\n",
      "|Michael|[Spark, Java]     |\n",
      "|Robert |[CSharp, VB]      |\n",
      "|Robert |[Spark, Python]   |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark – explode nested array into rows\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.subjects)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmpId: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+-----+------+\n",
      "|EmpId|Salary|\n",
      "+-----+------+\n",
      "|111  |50000 |\n",
      "|222  |60000 |\n",
      "|333  |40000 |\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+----------+\n",
      "|EmpId|Salary|lit_value1|\n",
      "+-----+------+----------+\n",
      "|111  |50000 |1         |\n",
      "|222  |60000 |1         |\n",
      "|333  |40000 |1         |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o430.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m df2\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m when\n\u001b[0;32m---> 17\u001b[0m df3 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlit_value2\u001b[39m\u001b[38;5;124m\"\u001b[39m, when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m,lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39motherwise(lit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m200\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     18\u001b[0m df3\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/column.py:111\u001b[0m, in \u001b[0;36m_bin_op.<locals>._\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m    110\u001b[0m     jc \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_jc \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Column) \u001b[39melse\u001b[39;00m other\n\u001b[0;32m--> 111\u001b[0m     njc \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jc, name)(jc)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(njc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o430.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "# typedLit() Function – Syntax\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "columns= [\"EmpId\",\"Salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col,lit\n",
    "df2 = df.select(col(\"EmpId\"),col(\"Salary\"),lit(\"1\").alias(\"lit_value1\"))\n",
    "df2.show(truncate=False)\n",
    "from pyspark.sql.functions import when\n",
    "df3 = df2.withColumn(\"lit_value2\", when(col(\"Salary\") >=40000 & col(\"Salary\") <= 50000,lit(\"100\")).otherwise(lit(\"200\")))\n",
    "df3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
